# Research: Operational Improvements for Clarity Loop

**ID**: R-008
**Created**: 2026-02-16
**Status**: Draft
**Author**: User + AI researcher

## Status

- **Research Type**: Evolutionary
- **Status**: draft
- **Open Questions**: 3 remaining
- **Discussion Rounds**: 1
- **Complexity**: L1-contained

## System Context

### Research Type: Evolutionary

This research evaluates six operational improvements identified in R-002 (Finding 5 and
Finding 7) for adoption in Clarity Loop. All six are polish/DX improvements to the existing
plugin architecture — none require architectural changes or new capabilities. They can be
adopted independently and incrementally.

### Related System Docs

| System Doc | Relevant Sections | Relationship |
|------------|-------------------|-------------|
| SYSTEM_DESIGN.md | §1 Architecture Overview (Plugin Structure) | Defines the 4-skill, 28-mode, 34-reference-file structure that these improvements operate within |
| SYSTEM_DESIGN.md | §7 State Management | Defines pipeline state files (.manifest.md, tracking files) that the auto-generated catalog would extend |
| SYSTEM_DESIGN.md | §8 Protection Model | Defines the hook-based protection (protect-system-docs.js, generate-manifest.js) that any new auto-generation must coexist with |
| SYSTEM_DESIGN.md | §15 Feedback Loops and Error Recovery | Defines existing error handling patterns that the graceful degradation finding would standardize |

### Current State

Clarity Loop is a Claude Code plugin with 4 skills, 28 modes, and 34 reference files. The
operational patterns examined here affect how modes are discovered, tested, documented, and
how they handle errors — cross-cutting concerns that touch all skills but don't change any
skill's core functionality.

**Mode discovery and registration**: Each SKILL.md has an explicit Mode Detection section
that lists all modes with their trigger phrases, gates, and descriptions. Adding a new mode
requires: (1) creating a reference file, (2) updating SKILL.md's Mode Detection section,
(3) updating SKILL.md's frontmatter description with trigger phrases, (4) adding a section
to SKILL.md describing the mode. The reference files follow a naming convention
(`{mode-name}-mode.md` for most, with exceptions like `bootstrap-guide.md`,
`proposal-template.md`, etc.) but this convention is not enforced or leveraged for
auto-discovery.

**Testability**: There is no way to invoke a reference file in isolation. Every invocation
goes through the full SKILL.md pipeline: configuration loading, pipeline state check, mode
detection, then reference file loading. Testing a mode means exercising the entire skill.

**Token awareness**: Heavy modes (audit reads ALL system docs, spec-mode reads all system
docs in parallel) have no explicit token budgeting. Modes consume whatever context they
need, and when context fills up, they degrade unpredictably rather than deliberately.

**Capability catalog**: The `.manifest.md` file (auto-generated by the PostToolUse hook)
indexes system docs — it does not index pipeline capabilities. No single document lists
all available modes, their inputs, outputs, prerequisites, and expected behavior.

**Error handling**: Each reference file describes error handling in prose, typically in a
dedicated section. The approach varies across files — some have structured error
classification (spec-mode's gap triage: L0/L1/L2), others have prose descriptions. There
is no standardized failure handling protocol across modes.

**Onboarding**: New users read the README and/or SYSTEM_DESIGN.md. Session Start in each
SKILL.md provides orientation (pipeline state check, brief summary), but there is no
interactive discovery command that shows overall pipeline state, all available modes, and
suggested next steps.

### Why This Research

R-002 identified these six operational patterns as emerged concepts (see R-002 Emerged
Concepts table). They were flagged as independent improvements that could be adopted
without the larger architectural changes (fan-out orchestration, browser automation, layered
delegation) that R-002 primarily investigated. This research scopes, evaluates, and designs
each improvement specifically for Clarity Loop.

The motivation is developer experience and maintainability:
- Mode discovery friction slows plugin development as mode count grows
- No isolated testability makes it hard to validate reference file changes
- No token awareness leads to unpredictable behavior in heavy modes
- No capability catalog makes it hard for users (and agents) to know what's available
- Inconsistent error handling produces inconsistent user experience
- No interactive discovery makes onboarding steeper than necessary

## Scope

### In Scope

- Evaluate 6 operational improvements for Clarity Loop adoption
- Design specific implementations for each
- Classify as "quick win" (implementable in a day) vs "investment" (needs its own proposal)
- For quick wins, provide enough detail for direct implementation
- For investments, scope the follow-up work

### Out of Scope

- Fan-out orchestration patterns (R-002 Phases 2-3 — separate proposal track)
- Browser automation capability (R-002 Phase 1 — separate proposal track)
- File structure conventions for reference files (R-002 Finding 6 — significant migration,
  separate research if pursued)
- Layered delegation architecture (R-002 Finding 1/4 — architectural change, not operational)

### Constraints

- Changes must be backward-compatible — no breaking changes to existing workflows
- New scripts/hooks must follow the existing patterns (hooks.json registration, config.js
  usage, init.js integration)
- Auto-generated files must be gitignored and regeneratable (like .manifest.md)
- SKILL.md remains the authoritative source for mode behavior — any auto-discovery must
  supplement, not replace, the explicit Mode Detection section

## Research Findings

### Finding 1: Directory-as-API for Mode Discovery

**Context**: Could modes be auto-discovered by naming convention instead of requiring
explicit SKILL.md updates when adding or removing modes?

**Analysis**: The current reference file naming conventions are:

| Skill | Reference Files | Naming Pattern |
|-------|----------------|----------------|
| cl-researcher | 6 files | Mixed: `bootstrap-guide.md`, `context-mode.md`, `document-plan-template.md`, `operational-bootstrap.md`, `proposal-template.md`, `research-template.md` |
| cl-reviewer | 9 files | Consistent: `audit-mode.md`, `correction-mode.md`, `design-review-mode.md`, `fix-mode.md`, `merge-mode.md`, `re-review-mode.md`, `review-mode.md`, `sync-mode.md`, `verify-mode.md` |
| cl-designer | 7 files | Mixed: `behavioral-walkthrough.md`, `build-plan-mode.md`, `design-checklist.md`, `mockups-mode.md`, `setup-mode.md`, `tokens-mode.md`, `visual-quality-rules.md` |
| cl-implementer | 10 files | Mixed: `autopilot-mode.md`, `cross-cutting-specs.md`, `governance-checks.md`, `operational-specs.md`, `run-mode.md`, `spec-consistency-check.md`, `spec-mode.md`, `start-mode.md`, `sync-mode.md`, `verify-mode.md` |

Only cl-reviewer follows a consistent `{mode-name}-mode.md` convention for all its mode
files. The other three skills mix mode files (`*-mode.md`) with auxiliary files (templates,
guides, checklists, rules). This means a simple glob for `*-mode.md` would miss modes
whose instructions live in non-mode-named files (like `bootstrap-guide.md` for bootstrap
mode) and would incorrectly include non-mode auxiliary files if they happened to match.

**The real problem with auto-discovery**: SKILL.md currently serves three roles for each mode:
1. **Router** — maps user input keywords to the correct mode
2. **Documenter** — describes what each mode does, its gates, and its expected behavior
3. **Orchestrator** — loads the reference file and sets up context (docsRoot, pipeline state)

Auto-discovery could replace role 1 (routing) but not roles 2 and 3. If a mode is added
by simply creating `references/new-mode.md`, SKILL.md would need to know:
- What trigger phrases activate this mode
- What prerequisite gates must be met
- What the mode does (for the Session Start orientation)
- How to describe it in the frontmatter (for Claude Code's skill matching)

None of these can be inferred from the filename alone. They could be specified in a
frontmatter block in the reference file itself, but this would split mode documentation
between two files (SKILL.md for general skill info, reference file for mode-specific info)
and make it harder to see all modes at a glance.

**Bowser comparison**: Bowser's directory-as-API works because its discoverable items (YAML
story files) are data, not behavior definitions. A story file says "test this URL with
these steps" — the orchestrator doesn't need to know anything about the story beyond its
filename. Clarity Loop's modes are behavioral instructions with gates, triggers, and
orchestration context — fundamentally different from data files.

**Alternative: auto-validation instead of auto-discovery**. Rather than auto-discovering
modes from the filesystem, a script could validate that every mode referenced in SKILL.md
has a corresponding reference file, and every `*-mode.md` file is referenced in SKILL.md.
This catches the mismatch without changing the registration model.

**Tradeoffs**:

| Approach | Pros | Cons |
|----------|------|------|
| Full auto-discovery | Zero-friction mode addition | Splits documentation, can't infer triggers/gates, breaks the "SKILL.md tells the full story" principle |
| Frontmatter in reference files | Self-describing modes, machine-parseable metadata | More complex, non-standard for Claude Code, still needs SKILL.md summary |
| Auto-validation (recommended) | Catches mismatches, preserves current model, low effort | Doesn't reduce mode-addition friction (still need SKILL.md edits) |

**Source**: Direct analysis of all 32 reference files across 4 skills, SKILL.md Mode
Detection sections, and Bowser's directory-as-API pattern from `ui-review.md`.

### Finding 2: Layer-Independent Testability

**Context**: Could reference files be invoked directly for testing, without going through
the full SKILL.md pipeline?

**Analysis**: Every SKILL.md reference file load follows the same pattern:

```
1. Read .clarity-loop.json → resolve docsRoot
2. Check for stale .pipeline-authorized marker
3. Read tracking files (RESEARCH_LEDGER, PROPOSAL_TRACKER, PARKING, DECISIONS)
4. Orient the user (2-3 sentence summary)
5. Detect mode from user arguments
6. Read reference file(s) for that mode
7. Execute mode instructions
```

Steps 1-5 are the "Session Start" preamble. Step 6-7 is the actual mode work. For testing,
steps 1-5 are overhead — a developer testing `verify-mode.md` doesn't care about the
pipeline state check or user orientation; they want to run the verification logic with
specific inputs.

**What would a reference file need to run standalone?** Examining the reference files, the
dependencies fall into three categories:

| Dependency | Examples | How to Provide Standalone |
|------------|----------|--------------------------|
| **Path configuration** | `docsRoot` — all files resolve paths relative to this | Environment variable or argument: `DOCS_ROOT=docs` |
| **Pipeline state** | Stale markers, tracker files, DECISIONS.md | Not needed for testing — testing validates mode logic, not pipeline state |
| **Mode-specific inputs** | Proposal file path (review), audit report path (correct), research doc path (proposal), spec file (sync) | Arguments passed directly |

The minimal standalone invocation would be:

```
claude "Read and follow /path/to/verify-mode.md. Context: docsRoot=docs,
proposal=P-001-AUTH.md. Skip pipeline state checks — this is a standalone test."
```

**The challenge**: Reference files contain instructions like "read PROPOSAL_TRACKER.md" and
"update DECISIONS.md" as part of their workflow — these are side effects that assume the
full pipeline context exists. A standalone test would either need to:
1. Have a fixture directory with all tracking files present (heavyweight)
2. Accept that standalone testing skips tracking updates (lightweight but incomplete)
3. Mock the tracking files with minimal content (middle ground)

**Design recommendation**: A lightweight test harness script that:
1. Creates a temp directory with minimal fixture files (empty tracking files, a sample
   `.clarity-loop.json`)
2. Copies or symlinks the target project's system docs (so mode logic has real content)
3. Invokes Claude Code with the reference file and test inputs
4. Reports the outcome

This is not a unit test framework — it's a manual testing aid. The developer runs it when
they want to validate a reference file change in isolation. It reduces the feedback loop
from "invoke skill, wait for pipeline checks, navigate to mode, observe behavior" to
"invoke mode directly with test inputs, observe behavior."

**Tradeoffs**:
- *Pro*: Dramatically faster feedback loop for reference file development
- *Pro*: Can test edge cases (e.g., "what does verify-mode do when the proposal has no
  Change Manifest?") without setting up the full pipeline state
- *Con*: Standalone tests don't exercise the Session Start integration — bugs in how
  SKILL.md passes context to the reference file would be missed
- *Con*: Requires maintaining test fixtures
- *Con*: Claude Code doesn't have a formal "run this markdown file as instructions" API —
  the standalone invocation relies on the model interpreting the reference file correctly
  without the SKILL.md context that normally frames it

**Source**: Analysis of Session Start sections in all 4 SKILL.md files and reference file
dependencies across all 32 reference files.

### Finding 3: Token-Aware Mode Design

**Context**: Could heavy modes declare their token appetite and offer tiers (light vs
thorough) based on available context?

**Analysis**: The heaviest modes in Clarity Loop are:

| Mode | Skill | What It Reads | Estimated Context |
|------|-------|--------------|-------------------|
| audit | cl-reviewer | ALL system docs (fresh, no manifest) + previous audit reports + external research | Very High — could exceed 200K context with large doc sets |
| spec | cl-implementer | ALL system docs (via subagent dispatch) + manifest + all tracking files | High — mitigated by subagent dispatch |
| verify (reviewer) | cl-reviewer | Proposal + all target system docs + manifest + previous reviews | Medium-High — scales with proposal size |
| verify (implementer) | cl-implementer | ALL specs + all completed task outputs + test results | High — scales with task count |

Currently, these modes have no awareness of their context consumption. They read as much
as they need and hope it fits. When context fills up, the model silently degrades —
producing less thorough analysis, skipping sections, or truncating output.

**Tiered mode design**: A mode could offer multiple tiers of thoroughness:

| Tier | When to Use | Behavior |
|------|------------|----------|
| **Quick** | Ship intent, routine check, known-good state | Read manifest only, spot-check 2-3 items, produce summary |
| **Standard** | Quality intent, normal workflow | Read relevant docs (manifest-guided), full analysis, standard report |
| **Thorough** | Rigor intent, pre-release, post-incident | Read ALL docs fresh, external research, comprehensive report |

**Integration with Guided Autonomy**: The Guided Autonomy proposal defines an intent system
(Ship/Quality/Rigor/Explore). Token-aware tiers map naturally:
- Ship intent → Quick tier (fast feedback, minimal context)
- Quality intent → Standard tier (balanced)
- Rigor intent → Thorough tier (comprehensive, context-heavy)

**How to declare tiers**: Two options were evaluated:

**Option A: Variables section in reference files**

```markdown
## Variables

TIER: keyword detection — "quick" | "standard" (default) | "thorough"
  - quick: manifest-only reads, spot-check 2-3 items
  - standard: manifest-guided targeted reads, full analysis
  - thorough: fresh reads of all docs, external research included
```

The mode instructions would then branch on TIER:

```markdown
### Phase 2: Read System Docs

If TIER = quick:
  Read manifest only. Select 2-3 highest-risk items for spot-check.

If TIER = standard:
  Read manifest. Targeted-read sections relevant to the proposal scope.

If TIER = thorough:
  Read ALL system docs fresh (bypass manifest). Include previous audit reports.
```

**Option B: Configuration in `.clarity-loop.json`**

```json
{
  "docsRoot": "docs",
  "modes": {
    "audit": { "defaultTier": "standard" },
    "verify": { "defaultTier": "standard" },
    "spec": { "defaultTier": "standard" }
  }
}
```

**Recommendation: Option A** (Variables in reference files) supplemented by intent-based
defaults. Rationale:
- The tier is a per-invocation choice, not a project-wide setting
- Users should be able to say `/cl-reviewer audit thorough` or `/cl-reviewer audit quick`
- The default maps from intent: Ship→quick, Quality→standard, Rigor→thorough
- Configuration override in `.clarity-loop.json` is available but optional

**What changes per tier**: Not just which docs to read, but also:
- **Analysis depth**: Quick skips external research, standard does targeted research,
  thorough does comprehensive research
- **Report detail**: Quick produces a summary table, standard produces full findings,
  thorough produces findings + cross-reference analysis + recommendations
- **Verification scope**: Quick spot-checks 2-3 items, standard checks all items in scope,
  thorough checks all items + related items outside explicit scope

**Tradeoffs**:
- *Pro*: Users get control over the time/thoroughness tradeoff
- *Pro*: Natural integration with Guided Autonomy intent system
- *Pro*: Heavy modes become usable in time-constrained situations
- *Con*: Three code paths per mode increase complexity and maintenance
- *Con*: Quick tier might miss real issues — users may develop false confidence
- *Con*: Tier choice is a judgment call — what if the user picks Quick when they need
  Thorough?

**Mitigation for the "wrong tier" risk**: The mode can escalate. If Quick tier spot-checks
reveal issues, the mode says: "Quick scan found 2 issues in the 3 items checked. Recommend
re-running at Standard or Thorough tier to assess full scope." This is advisory, not
automatic — the user decides whether to escalate.

**Source**: Analysis of context consumption patterns across the 4 heaviest modes, Bowser's
opt-in vision mode as a token-aware design precedent, and the Guided Autonomy intent system.

### Finding 4: Auto-Generated Pipeline Capability Catalog

**Context**: Could a script generate a catalog of all available modes, their inputs,
outputs, and dependencies?

**Analysis**: Bowser's `/list-tools` command generates `TOOLS.md` by introspecting the
system prompt and extracting all available tools as TypeScript function prototypes. The
result is a current, machine-generated reference document.

Clarity Loop's `.manifest.md` (auto-generated by `generate-manifest.js`) serves a similar
purpose for system docs but not for pipeline capabilities. There is no single document
that answers: "What modes are available? What does each one do? What inputs does it need?
What gates must be met?"

**What the catalog would contain**: For each mode across all 4 skills:

```markdown
## cl-reviewer

### audit
- **Trigger phrases**: "audit", "system doc health", "check consistency"
- **Prerequisites**: System docs must exist
- **Inputs**: None required (operates on entire system doc set)
- **Outputs**: `{docsRoot}/reviews/audit/AUDIT_[YYYY-MM-DD].md`
- **Reference file**: `skills/cl-reviewer/references/audit-mode.md`
- **Side effects**: May update PARKING.md with emerged concepts
- **Estimated weight**: Heavy (reads all system docs + external research)
```

**How to generate it**: The catalog could be generated by parsing SKILL.md files. The
information is already there — it just needs extraction:

1. **Parse frontmatter** — extract `name`, `description`, `argument-hint`
2. **Parse Mode Detection section** — extract mode names, trigger phrases, and gates
3. **Parse per-mode sections** — extract the brief description and reference file path
4. **Parse reference file headers** — extract any additional metadata (inputs, outputs)
5. **Assemble** into a structured markdown document

**Implementation approach**: A new script, `scripts/generate-catalog.js`, following the
same patterns as `hooks/generate-manifest.js`:

- Reads all 4 `skills/*/SKILL.md` files
- Parses Mode Detection sections using regex patterns (headings, bullet lists)
- Extracts reference file paths from per-mode sections
- Checks reference file existence (validation)
- Generates `{docsRoot}/PIPELINE_CATALOG.md`
- Gitignored (auto-generated, not source of truth)

**Generation triggers**:
- Manually: `node scripts/generate-catalog.js`
- During init: called by `scripts/init.js` after directory scaffolding
- Optionally: as a PostToolUse hook on SKILL.md edits (low priority — SKILL.md changes
  are infrequent)

**Secondary benefit — mode validation**: As a side effect of parsing, the script can
validate:
- Every mode in SKILL.md's Mode Detection references a reference file that exists
- Every `*-mode.md` reference file is referenced by a SKILL.md
- Frontmatter `argument-hint` lists match the Mode Detection section modes
- No duplicate mode names across skills

This is the "auto-validation" recommended in Finding 1 — it comes free with the catalog
generation.

**Tradeoffs**:
- *Pro*: Single source of truth for "what can the pipeline do?"
- *Pro*: Useful for onboarding, debugging, and agent self-orientation
- *Pro*: Free mode validation as a side effect
- *Pro*: Low implementation effort (similar complexity to generate-manifest.js)
- *Con*: Another auto-generated file to maintain
- *Con*: Parsing SKILL.md sections is fragile — format changes break the parser
- *Con*: The catalog duplicates information that's already in SKILL.md (but in a more
  accessible format)

**Source**: Analysis of Bowser's `/list-tools` command and `TOOLS.md` output, Clarity
Loop's `generate-manifest.js` implementation, and all 4 SKILL.md file structures.

### Finding 5: Graceful Degradation Patterns

**Context**: Could error handling be standardized across modes with a structured failure
handling protocol?

**Analysis**: Current error handling varies significantly across reference files:

| Reference File | Error Handling Approach |
|---------------|----------------------|
| `audit-mode.md` | Prose description: "If a doc cannot be read, note it and continue" |
| `spec-mode.md` | Structured gap triage: L0 (patch inline), L1 (log + ask), L2 (pause + research) |
| `run-mode.md` | Fix task protocol: create F-NNN task, prioritize over new tasks |
| `verify-mode.md` | Findings classification: blocking vs non-blocking vs advisory |
| `merge-mode.md` | Pre-apply validation: confirm targets still match, impact assessment |
| `autopilot-mode.md` | Self-test loop: test failure → fix → retest → escalate if stuck |

The `spec-mode.md` gap triage (L0/L1/L2) is the most structured pattern, but it's specific
to spec gaps. The `run-mode.md` fix task protocol is well-defined but specific to
implementation failures. There's no universal failure classification.

**Proposed failure classification**: A standard taxonomy that all modes can use:

| Classification | Meaning | Action | Example |
|---------------|---------|--------|---------|
| **Skip** | This item failed but doesn't block others | Log warning, continue with remaining items | Audit: one doc unreadable → skip it, note in report |
| **Partial** | Completed with reduced quality | Note degradation, continue, flag in output | Review: one dimension couldn't be checked → note as "not verified" |
| **Retry** | Transient failure, worth retrying | Retry once, then escalate to Partial or Stop | Web search timeout → retry once → if still failing, skip external research |
| **Stop** | Cannot continue meaningfully | Stop current phase, report what was completed, suggest remediation | Merge: target file doesn't match assumptions → stop, show diff, ask user |
| **Escalate** | Needs user decision | Present the issue with options, wait for user input | Spec gap L1: ambiguous requirement → present options, user decides |

**How to apply**: Each mode's workflow steps would include explicit failure handling:

```markdown
### Step 3: Read system docs

For each system doc in the manifest:
  Read the file.
  On failure (file not found or unreadable):
    Classification: Skip
    Action: Log "Could not read [filename]: [reason]" in the report's
    Warnings section. Continue with remaining docs.
    Escalation: If >50% of docs fail to read, reclassify as Stop.
```

**This is NOT a rewrite of all reference files.** The recommendation is to:
1. Define the failure classification taxonomy in a shared reference file
   (`references/failure-protocol.md` or similar)
2. Reference it from heavy modes that currently lack structured error handling
3. Adopt incrementally — when a reference file is updated for any reason, add structured
   failure handling to its workflow steps

**Tradeoffs**:
- *Pro*: Consistent user experience across modes — failures always classified the same way
- *Pro*: Machine-parseable failure output (classification + action taken + impact)
- *Pro*: Explicit escalation thresholds prevent silent degradation
- *Con*: Overhead for simple modes that rarely fail (e.g., status mode)
- *Con*: The taxonomy adds cognitive load for reference file authors
- *Con*: Some failures are genuinely mode-specific and don't fit a generic classification

**Mitigation**: The taxonomy is a guideline, not a mandate. Simple modes (status, triage)
don't need it. Complex modes (audit, spec, merge, verify) benefit most. The adoption
should be pragmatic — if a mode's existing error handling is clear and sufficient, don't
force it into the taxonomy.

**Source**: Comparative analysis of error handling patterns across all 32 reference files,
Bowser's per-layer failure handling (YAML parse → skip, agent timeout → mark FAIL,
step failure → capture + SKIPPED remaining), and Clarity Loop's spec-mode gap triage
as an existing structured pattern.

### Finding 6: Onboarding/Discovery Flow

**Context**: Could an interactive discovery command show pipeline state, available modes,
recent activity, and suggested next steps?

**Analysis**: Currently, each SKILL.md's Session Start section provides a brief orientation
when that skill is invoked. But there's no way to get an overview of the entire pipeline
without invoking a specific skill. A user asking "where are we?" has to:
1. Remember which skill to invoke
2. Wait for that skill's pipeline state check
3. Mentally merge orientation from multiple skills if they want the full picture

**What a discovery command would show**:

```
Pipeline Status
===============
Project: My App (docsRoot: docs/)
System docs: 5 files (last modified: 2026-02-14)
Manifest: current (hash: a1b2c3d4)

Active Work
-----------
Research: R-008 OPERATIONAL_IMPROVEMENTS (status: draft)
Proposals: P-005 CONTEXT_SYSTEM (status: approved, ready for merge)
Implementation: TASKS.md exists, 8/15 tasks complete
Designs: DESIGN_PROGRESS.md exists, mockups complete

Available Modes
---------------
  /cl-researcher  bootstrap | research | structure | proposal | context
  /cl-reviewer    review | re-review | fix | merge | verify | audit | correct | sync | design-review
  /cl-designer    setup | tokens | mockups | build-plan
  /cl-implementer spec | spec-review | start | run | autopilot | verify | status | sync

Suggested Next Steps
--------------------
  1. P-005 is approved — run /cl-reviewer merge P-005-CONTEXT_SYSTEM.md
  2. R-008 is in draft — continue discussion or mark approved
  3. Implementation at 53% — run /cl-implementer run to continue

Recent Decisions
----------------
  D-012: Auth tokens use httpOnly cookies (2026-02-13, category: auth)
  D-011: Error responses use RFC 7807 format (2026-02-12, category: errors)
```

**Implementation options**:

**Option A: A new skill mode** (e.g., `/cl-researcher status` or a dedicated skill)
- Requires a new mode in SKILL.md + reference file
- Full SKILL.md overhead (Session Start, mode detection)
- But the Session Start IS the status check — this mode would essentially be
  "Session Start, but don't do anything else"

**Option B: A script** (`scripts/pipeline-status.js`)
- Runs outside Claude Code, produces plain text output
- Fast (no LLM invocation)
- Can be called from init.js or independently
- But can't provide the "Suggested Next Steps" intelligence — that requires LLM reasoning

**Option C: A Claude Code slash command** (a `.md` file in `commands/`)
- Lightweight — just a markdown file that says "read tracking files, generate status"
- No SKILL.md overhead — commands are simpler than skill modes
- Can provide intelligent suggestions via LLM reasoning
- Caveat: Clarity Loop is a plugin with skills, not a project with commands — commands
  live in `.claude/commands/` which is project-level, not plugin-level

**Recommended: Option B (script) + Option A (skill mode)**

The script (`scripts/pipeline-status.js`) generates the factual status report:
- Parse `.clarity-loop.json` for docsRoot
- Read `docs/system/.manifest.md` for system doc state
- Read `RESEARCH_LEDGER.md` for active research
- Read `PROPOSAL_TRACKER.md` for proposal state
- Check for `TASKS.md` and parse progress
- Check for `DESIGN_PROGRESS.md` and parse state
- Read last 3 entries from `DECISIONS.md`
- Output structured text

The skill mode (adding `status` to cl-researcher's Mode Detection) wraps the script
output with LLM intelligence:
- Interpret the factual status
- Generate contextual suggestions ("P-005 is approved — you could merge it")
- Identify potential issues ("Specs were generated 5 days ago but system docs changed since")

This two-layer approach gives both: fast factual status (script) and intelligent
recommendations (skill mode).

**Tradeoffs**:
- *Pro*: Single place to see the full pipeline state
- *Pro*: Intelligent suggestions based on current state
- *Pro*: Script output can be used by other tools (CI, dashboards)
- *Con*: Two implementations to maintain (script + mode)
- *Con*: The script must parse multiple tracking file formats (fragile)
- *Con*: Adding a mode to cl-researcher increases its already-large scope (7 modes → 8)

**Source**: Analysis of Bowser's `/prime` command (onboarding/discovery), all 4 SKILL.md
Session Start sections, and the existing tracking file formats.

## Options Analysis

The six improvements are independent. Rather than choosing between them, the analysis
evaluates each on an effort/impact matrix to prioritize adoption order.

| Improvement | Impact | Effort | Category | Priority |
|------------|--------|--------|----------|----------|
| **Auto-generated catalog** (Finding 4) | High — fills a real gap, useful for users AND agents | Low — similar to existing generate-manifest.js | Quick win | 1 |
| **Onboarding/discovery** (Finding 6) | High — dramatically improves new-user experience | Medium — script is low effort, skill mode adds complexity | Quick win (script) + Investment (mode) | 2 |
| **Graceful degradation** (Finding 5) | Medium — improves reliability and UX of heavy modes | Low — define taxonomy, adopt incrementally | Quick win (taxonomy) + Incremental (adoption) | 3 |
| **Token-aware tiers** (Finding 3) | Medium — useful for heavy modes, integrates with intent | Medium — requires changes to 4-5 reference files | Investment | 4 |
| **Auto-validation** (Finding 1) | Low-Medium — catches mismatches, improves maintainability | Low — side effect of catalog generation | Quick win (bundled with Finding 4) | Bundled |
| **Testability harness** (Finding 2) | Medium — faster dev feedback loop | Medium — fixture setup, no formal API for reference file invocation | Investment | 5 |

## Recommendations

### Primary Recommendation: Phased Adoption by Priority

Adopt the improvements in three waves, ordered by ROI (impact divided by effort):

**Wave 1 — Quick Wins (can implement in a day each)**

1. **Auto-generated pipeline catalog** (Finding 4): New `scripts/generate-catalog.js` that
   parses all SKILL.md files, extracts mode information, and generates
   `{docsRoot}/PIPELINE_CATALOG.md`. Include mode validation as a side effect. Add to
   init.js. Gitignore the output.

2. **Pipeline status script** (Finding 6, script half): New `scripts/pipeline-status.js`
   that reads all tracking files and produces a structured text status report. Callable
   standalone or from init.js.

3. **Failure classification taxonomy** (Finding 5, definition only): New shared reference
   file `skills/shared/failure-protocol.md` defining the Skip/Partial/Retry/Stop/Escalate
   taxonomy with examples. Referenced by heavy modes as they are updated.

**Wave 2 — Targeted Investments (need focused implementation sessions)**

4. **Token-aware tiers for audit and verify modes** (Finding 3): Add Variables sections to
   `audit-mode.md` and `verify-mode.md` (both skills) with Quick/Standard/Thorough tier
   definitions. Integrate with Guided Autonomy intent system when it lands. Start with the
   two heaviest modes, expand if the pattern proves useful.

5. **Onboarding skill mode** (Finding 6, mode half): Add `status` mode to cl-researcher
   SKILL.md that wraps the pipeline-status script output with intelligent suggestions. This
   becomes the recommended first command for new users: `/cl-researcher status`.

**Wave 3 — If Needed (evaluate after Waves 1-2)**

6. **Testability harness** (Finding 2): Build a lightweight test runner for reference files
   if reference file development velocity justifies it. The need may be reduced by Wave 1
   improvements (catalog validation catches structural issues, status script catches state
   issues). Revisit after 3-5 reference file changes to assess whether the feedback loop
   is still too slow.

**Not recommended**:

7. **Full directory-as-API auto-discovery** (Finding 1): The analysis shows this trades
   clarity (SKILL.md as complete mode documentation) for convenience (no SKILL.md edit
   when adding modes). The convenience gain is small (modes are added infrequently — the
   system has had 28 modes since initial design) and the clarity loss is significant.
   Auto-validation (bundled with the catalog script) provides the main benefit without
   the cost.

### Risks & Mitigations

| Risk | Likelihood | Impact | Mitigation |
|------|-----------|--------|------------|
| SKILL.md parsing is fragile — format changes break the catalog generator | Medium | Low — catalog is a convenience, not a dependency | Pin expected format patterns in the generator; warn on parse failures instead of crashing |
| Failure taxonomy adds overhead without improving behavior — modes ignore it | Medium | Low — the taxonomy is a guideline, not enforced | Adopt incrementally in heavy modes only; don't mandate for simple modes |
| Token-aware tiers create false confidence — Quick tier misses real issues | Medium | Medium — user thinks system is healthy when it isn't | Quick tier always includes escalation advice: "N issues found in M spot-checks. Consider re-running at Standard tier." |
| Status mode adds scope to cl-researcher (7→8 modes) | Low | Low — status is a read-only mode with no side effects | Keep it minimal — read tracking files, format output, suggest next steps. No conversation, no file creation. |
| Pipeline status script requires parsing multiple file formats | Medium | Low — tracking files are simple markdown tables | Use the same regex patterns the existing init.js and manifest scripts use |

### Impact on System Docs

| System Doc | Expected Changes |
|------------|-----------------|
| SYSTEM_DESIGN.md | §1 Plugin Structure — add catalog script and status script to the Scripts subgraph. §8 Protection Model — add catalog generation to hook/script inventory. §17 File Inventory — add PIPELINE_CATALOG.md and pipeline-status.js. |
| SYSTEM_DESIGN.md | §14 Verification and Audit — add failure classification taxonomy reference. §15 Feedback Loops — add tier-based degradation pattern. |

These are incremental additions to existing sections — no new sections or restructuring
required.

## Decision Log

| # | Topic | Considered | Decision | Rationale |
|---|-------|-----------|----------|-----------|
| 1 | Auto-discovery vs auto-validation | Full directory-as-API (modes auto-discovered from filenames) vs validation script (checks SKILL.md ↔ filesystem consistency) | Auto-validation only, bundled with catalog generation | SKILL.md's documentation role is more valuable than zero-friction mode addition. Modes are added infrequently (~28 modes have been stable). Auto-validation catches the actual pain point (mismatches) without losing SKILL.md's "single source of mode truth" property. |
| 2 | Token tier declaration mechanism | Reference file Variables section vs .clarity-loop.json config | Variables in reference files with intent-based defaults | Tier is a per-invocation choice, not a project setting. Users should be able to say "audit thorough" for one run and "audit quick" for the next. Config can set defaults but invocation overrides. |
| 3 | Onboarding approach | Script only vs skill mode only vs both | Both: script for factual status + skill mode for intelligent suggestions | Script is fast and tool-friendly (CI, dashboards). Skill mode adds LLM intelligence for contextual suggestions. The script runs in milliseconds; the skill mode adds agent reasoning. Two complementary layers. |
| 4 | Failure taxonomy scope | Universal (all modes) vs heavy modes only | Heavy modes only, with optional adoption for others | Simple modes (status, triage) rarely fail and don't need structured failure handling. The taxonomy adds value where failures are complex and cascading (audit, merge, spec, verify). Mandating it everywhere adds overhead without benefit. |
| 5 | Catalog generation trigger | PostToolUse hook on SKILL.md edits vs manual only vs init-time | Init-time + manual invocation. No hook. | SKILL.md edits are rare enough that a hook is overkill. The catalog needs to be current at init time (for onboarding) and regeneratable on demand. Hooking PostToolUse for SKILL.md edits would also require the hook to detect whether the edited file is a SKILL.md — complexity not justified by the use case. |

## Emerged Concepts

| Concept | Why It Matters | Suggested Action |
|---------|---------------|-----------------|
| Shared reference files across skills | The failure protocol (`failure-protocol.md`) would be the first cross-skill shared reference file. Currently each skill has its own `references/` directory with no sharing. A `skills/shared/` directory could hold cross-cutting reference files (failure protocol, common Variables patterns, output format templates). | Evaluate during Wave 1 implementation. If other cross-skill references emerge, formalize the `skills/shared/` convention. If failure-protocol remains the only one, it can live in any skill's references with a comment noting it's cross-skill. |
| Intent-tier integration protocol | The mapping from Guided Autonomy intents (Ship/Quality/Rigor) to token-aware tiers (Quick/Standard/Thorough) needs a formal protocol — who reads the intent, how is it passed to the mode, what happens when the user explicitly overrides. This is a coordination point between the Guided Autonomy proposal and this research. | Defer to the Guided Autonomy proposal. Note the dependency: when intent system lands, tier defaults should integrate with it. Until then, tiers default to Standard. |
| Tracking file format stability | Both the catalog generator and the status script depend on parsing tracking file formats (RESEARCH_LEDGER.md, PROPOSAL_TRACKER.md, etc.). If these formats change, both scripts break. A lightweight schema or format contract for tracking files would reduce this fragility. | Low priority. The tracking file formats have been stable since bootstrap. If they change, update the parsers at the same time. Don't over-engineer format contracts for files that change rarely. |

## Open Questions

1. **Shared references directory**: Should cross-skill reference files live in
   `skills/shared/references/`, in each skill's directory with a cross-reference comment,
   or somewhere else? This affects Finding 5's failure protocol placement. The decision
   may depend on whether more cross-skill references emerge.

2. **Status mode naming**: Adding `status` to cl-researcher puts an 8th mode on an
   already-large skill. Alternatively, it could be a standalone script-only solution (no
   skill mode) or a mode on a different skill. Which approach fits best?

3. **Catalog freshness**: Should the catalog have a staleness indicator (like the manifest's
   content hash) so users know if it was generated before the latest SKILL.md change?
   This matters for onboarding — a stale catalog could mislead a new user.

## References

- R-002 Research: Bowser Architecture Patterns — Finding 5 (Keyword Config), Finding 7
  (Operational Design Patterns), and Emerged Concepts table
  (`docs/research/R-002-BOWSER_ARCHITECTURE_PATTERNS.md`)
- Bowser `/list-tools` command: `/Users/bhushan/Documents/bowser/.claude/commands/list-tools.md`
- Bowser `/prime` command: `/Users/bhushan/Documents/bowser/.claude/commands/prime.md`
- Bowser `/build` command: `/Users/bhushan/Documents/bowser/.claude/commands/build.md`
- Clarity Loop SYSTEM_DESIGN.md: `docs/SYSTEM_DESIGN.md` — §1 Architecture Overview,
  §7 State Management, §8 Protection Model, §15 Feedback Loops
- Clarity Loop hooks: `hooks/generate-manifest.js` (auto-generation pattern),
  `hooks/protect-system-docs.js` (protection pattern), `hooks/hooks.json` (registration)
- Clarity Loop scripts: `scripts/init.js` (scaffolding pattern)
- All 4 SKILL.md files: mode detection sections, session start patterns, reference file
  loading conventions
- All 32 reference files: error handling patterns, dependency analysis, naming conventions
